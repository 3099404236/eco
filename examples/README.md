# 为什么要平方残差？直观演示

## 概述

这个演示回答了计量经济学中的一个关键问题：**为什么OLS（普通最小二乘法）要最小化残差平方和，而不是简单地最小化残差和？**

## 快速开始

运行演示：
```bash
python3 why_squared_residuals.py
```

这将生成一个包含4个子图的可视化文件：`why_squared_residuals.png`

## 关键洞察

### 问题：为什么不能直接最小化 Σeᵢ？

考虑这个例子（来自教材）：
- 真实值 Y₁ = 100,000，预测值 = 1,100,000，误差 = **-1,000,000**
- 真实值 Y₂ = 100,000，预测值 = -900,000，误差 = **+1,000,000**

如果直接求和：
```
Σeᵢ = (-1,000,000) + (+1,000,000) = 0
```

看起来误差为零，完美！但实际上这个模型预测得**一塌糊涂**。

### 解决方案：平方残差

```
Σeᵢ² = (-1,000,000)² + (+1,000,000)² = 2,000,000,000,000
```

平方后：
1. ✅ **正负误差不会抵消** - 所有误差都变成正数
2. ✅ **大误差被重罚** - 误差越大，平方后的值增长越快
3. ✅ **有唯一解** - OLS有解析解，计算高效

## 演示内容

脚本比较了三种优化方法：

### 1. 最小化残差和 Σeᵢ（有缺陷）
```python
minimize: Σeᵢ = Σ(Yᵢ - β₀ - β₁Xᵢ)
```
**问题：** 正负残差相互抵消，可能找到荒谬的拟合直线

### 2. 最小化残差平方和 Σeᵢ²（OLS标准方法）
```python
minimize: Σeᵢ² = Σ(Yᵢ - β₀ - β₁Xᵢ)²
```
**优点：**
- 正负误差都被平方，不会抵消
- 大误差被"惩罚"得更重
- 有唯一的解析解

### 3. 最小化残差绝对值和 Σ|eᵢ|（LAD方法）
```python
minimize: Σ|eᵢ| = Σ|Yᵢ - β₀ - β₁Xᵢ|
```
**特点：** 对离群点更稳健，但计算更复杂

## 可视化说明

生成的图表包含4个子图：

1. **三种拟合方法对比**：展示不同优化目标得到的拟合直线
2. **OLS残差分布**：展示残差的正负分布（绿色为正，橙色为负）
3. **为什么不能直接求和**：用具体数字展示正负误差抵消的问题
4. **残差 vs 残差平方**：展示平方如何放大大误差的影响

## 数学原理

### OLS估计量

最小化残差平方和：
```
min Σ(Yᵢ - β₀ - β₁Xᵢ)²
```

解析解（通过求导得到）：
```
β₁ = Σ(Xᵢ - X̄)(Yᵢ - Ȳ) / Σ(Xᵢ - X̄)²
β₀ = Ȳ - β₁X̄
```

### 为什么平方是特殊的？

1. **数学优雅性**：平方函数是凸函数，保证有唯一全局最优解
2. **统计性质**：在经典假设下，OLS估计量是BLUE（最佳线性无偏估计量）
3. **计算效率**：有解析解，不需要数值优化
4. **几何解释**：最小化欧几里得距离

## 实验结果示例

使用30个样本点（包含3个离群点）：

| 方法 | 拟合结果 | 残差平方和 |
|------|---------|-----------|
| 残差和 | Y = 2568931517.59 + 12844802416.66X | 1.78×10²³ |
| **残差平方和 (OLS)** | **Y = 11.17 + 1.84X** | **778.78** |
| 残差绝对值和 (LAD) | Y = 10.99 + 1.77X | 787.88 |

真实关系：Y = 10 + 2X

可以看到，OLS方法得到了最接近真实参数的估计！

## 依赖项

```bash
pip install numpy matplotlib scipy
```

## 参考资料

这个演示基于经典计量经济学教材第2章"普通最小二乘法"的内容。

## 进一步思考

1. **为什么不总是用LAD？** 虽然LAD对离群点更稳健，但OLS在正态分布假设下是最优的
2. **其他损失函数？** Huber损失、分位数回归等提供了其他选择
3. **何时用哪个？** 取决于数据特征和研究目标

---

**关键结论：平方残差不是任意的选择，而是基于坚实的数学和统计理论！**
