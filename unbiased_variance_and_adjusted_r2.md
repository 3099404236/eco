# 从方差无偏估计到调整R²

## 核心问题：为什么样本方差要除以(N-1)？

### 1. 两种方差估计

**有偏估计（除以N）：**
```
s² = Σ(Yᵢ - Ȳ)² / N
```

**无偏估计（除以N-1）：**
```
s² = Σ(Yᵢ - Ȳ)² / (N-1)
```

### 2. 蒙特卡洛模拟的实证结果

我们从一个已知方差σ²=225的总体中抽样10,000次，结果显示：

| 样本量N | 真实方差 | 有偏估计均值 | 偏差% | 无偏估计均值 | 偏差% |
|---------|----------|--------------|-------|--------------|-------|
| 5       | 225      | 180.55       | -19.76% | 225.68     | +0.30% |
| 10      | 225      | 202.61       | -9.95%  | 225.13     | +0.06% |
| 30      | 225      | 217.19       | -3.47%  | 224.68     | -0.14% |
| 100     | 225      | 222.81       | -0.97%  | 225.06     | +0.03% |

**关键发现：**
- **有偏估计系统性地低估真实方差**，且样本量越小，低估越严重
- **无偏估计非常接近真实方差**，平均偏差接近0%
- 理论偏差 = -σ²/N，与模拟结果完美吻合

### 3. 数学证明

关键是理解 **E[Σ(Yᵢ - Ȳ)²]** 的期望值：

```
E[Σ(Yᵢ - Ȳ)²] = E[Σ(Yᵢ - μ + μ - Ȳ)²]
                = E[Σ(Yᵢ - μ)²] - N·E[(Ȳ - μ)²]
                = N·σ² - N·(σ²/N)
                = N·σ² - σ²
                = (N-1)·σ²
```

因此：
- **有偏估计**：E[Σ(Yᵢ - Ȳ)² / N] = (N-1)·σ² / N = σ²·(N-1)/N ≠ σ²
- **无偏估计**：E[Σ(Yᵢ - Ȳ)² / (N-1)] = (N-1)·σ² / (N-1) = σ² ✓

### 4. 直观理解：为什么会低估？

用样本均值Ȳ代替总体均值μ计算离差时：
- **Ȳ是使Σ(Yᵢ - Ȳ)²最小的点**（最小二乘原理）
- 相比用真实均值μ，用Ȳ计算的离差平方和会更小
- 这导致系统性地低估方差
- 除以(N-1)而不是N，正好修正了这个偏差

### 5. 自由度的含义

**自由度 df = N - 1** 的含义：

当你计算Σ(Yᵢ - Ȳ)²时：
1. 你先用N个观测值计算了样本均值Ȳ
2. 这"用掉"了1个自由度
3. 实际上只剩下(N-1)个独立信息

**数学上的约束**：Σ(Yᵢ - Ȳ) = 0

这意味着：
- 知道前(N-1)个离差后，最后一个离差就被确定了
- 真正"自由"的只有(N-1)个离差

**示例（N=5）：**
```
离差：-5, +2, -12, +10, +5
前4个离差可以任意，但第5个必须是+5（使总和=0）
```

---

## 从方差估计到调整R²

### 回顾：普通R²的定义

```
R² = 1 - SSR/SST = 1 - Σ(Yᵢ - Ŷᵢ)² / Σ(Yᵢ - Ȳ)²
```

### 问题：R²会随变量数增加而虚高

即使添加无关变量，R²也不会下降，因为：
- SSR只会减少（或不变）
- R²会机械性地上升

### 调整R²的原理

**调整R²用方差代替平方和：**

```
R̄² = 1 - (SSR/(N-K-1)) / (SST/(N-1))
   = 1 - (σ̂ε²) / (σ̂y²)
```

其中：
- **σ̂ε² = SSR/(N-K-1)** = 残差方差的无偏估计
- **σ̂y² = SST/(N-1)** = Y方差的无偏估计
- K = 解释变量个数

### 关键洞察

1. **调整R²的分子分母都是无偏的方差估计**
   - 分子：SSR/(N-K-1)，自由度 = N-K-1（估计了K+1个参数：K个斜率+1个截距）
   - 分母：SST/(N-1)，自由度 = N-1（估计了1个参数：均值）

2. **为什么能惩罚多余变量？**
   - 添加变量K增加 → 分子的自由度(N-K-1)减少
   - 即使SSR略微下降，σ̂ε²可能反而上升
   - 因为你在用更少的自由度来估计方差

3. **数学上的等价形式**

```
R̄² = 1 - (1 - R²)·(N-1)/(N-K-1)
```

这个公式清楚地显示：
- 如果K增加但R²增加不够，R̄²会下降
- 这正是对"无效变量"的惩罚

---

## 总结：完整的逻辑链条

```
方差无偏估计需要除以自由度
         ↓
在回归中，残差自由度 = N-K-1
         ↓
调整R²用无偏方差估计代替平方和
         ↓
这自动引入了对模型复杂度的惩罚
         ↓
只有"足够好"的变量才能让R̄²上升
```

**关键教训：**
> 调整R²不是一个人为的修正，而是正确应用无偏方差估计的自然结果。它尊重了自由度的概念，确保我们在比较不同模型时使用的是"苹果对苹果"的比较。

---

## 代码示例

运行以下脚本查看完整演示：
1. `variance_bias_simulation.py` - 蒙特卡洛模拟（10,000次重复）
2. `variance_example.py` - 单个样本的详细计算

**核心结论：**
- 除以N会系统性低估方差约 (1/N)×100%
- 除以(N-1)是无偏的，这是统计学的基本原理
- 这个原理直接导致了调整R²的形式

---

## 扩展阅读

**为什么这很重要？**

1. **模型选择**：调整R²帮助你在拟合优度和模型复杂度之间权衡
2. **过拟合预防**：它惩罚那些只是"拟合噪声"的变量
3. **理论一致性**：它基于坚实的统计理论（无偏估计）
4. **实践指导**：当R²上升但R̄²下降时，你应该重新考虑添加的变量

**注意事项：**
- 调整R²可能为负（当模型非常差时）
- 它仍然不是完美的模型选择标准（考虑AIC、BIC等）
- 但它比普通R²更可靠，尤其是在比较不同变量数的模型时
